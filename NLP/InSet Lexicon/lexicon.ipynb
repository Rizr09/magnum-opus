{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score: 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def load_lexicon(file_path):\n",
    "    lexicon = pd.read_csv(file_path, sep='\\t', header=0, names=['word', 'weight'])\n",
    "    lexicon['weight'] = pd.to_numeric(lexicon['weight'], errors='coerce')\n",
    "    lexicon['length'] = lexicon['word'].apply(lambda x: len(x.split()))\n",
    "    return lexicon.sort_values(by='length', ascending=False).reset_index(drop=True)\n",
    "\n",
    "positive_lexicon = load_lexicon('positive.tsv')\n",
    "negative_lexicon = load_lexicon('negative.tsv')\n",
    "\n",
    "def calculate_sentiment(text, positive_lexicon, negative_lexicon):\n",
    "    text = preprocess_text(text)\n",
    "    sentiment_score = 0\n",
    "    words = text.split()\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        for j in range(len(words), i, -1):\n",
    "            phrase = ' '.join(words[i:j])\n",
    "            \n",
    "            pos_match = positive_lexicon[positive_lexicon['word'] == phrase]\n",
    "            if not pos_match.empty:\n",
    "                sentiment_score += pos_match.iloc[0]['weight']\n",
    "                words[i:j] = [''] * (j-i)\n",
    "                break\n",
    "            \n",
    "            neg_match = negative_lexicon[negative_lexicon['word'] == phrase]\n",
    "            if not neg_match.empty:\n",
    "                sentiment_score += neg_match.iloc[0]['weight']\n",
    "                words[i:j] = [''] * (j-i)\n",
    "                break\n",
    "    \n",
    "    return sentiment_score\n",
    "\n",
    "# Example usage\n",
    "example_text = \"jalan terbuka putus tali gantung\"\n",
    "sentiment_score = calculate_sentiment(example_text, positive_lexicon, negative_lexicon)\n",
    "print(f\"Sentiment score: {sentiment_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Data/News/combined_data_sorted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply calculate_sentiment to all rows and save the result in a new column called 'sentiment_score_lexicon'\n",
    "df['Lexicon Sentiment Score'] = df['Title'].apply(lambda x: calculate_sentiment(x, positive_lexicon, negative_lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Lexicon Sentiment Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Mengenal Lagi Naturalisasi, Cara Anies Basweda...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Streaming! Upaya Sarinah Menjadi Pusat Pengemb...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Terungkap! Pembeli Pejaten Village Juga Invest...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>Perhatikan 5 Pertanyaan Ini Sebelum Merancang ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>Saham Blue Chip Bisa Digoreng Juga?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5420</th>\n",
       "      <td>2024-09-25</td>\n",
       "      <td>IHSG Diprediksi Menguat, Simak Analisis dan Re...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5421</th>\n",
       "      <td>2024-09-25</td>\n",
       "      <td>Bikin Investor Happy, The Fed Diprediksi Bakal...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>2024-09-26</td>\n",
       "      <td>Suku Bunga Turun, Kemenperin Optimis Iklim Usa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5423</th>\n",
       "      <td>2024-09-26</td>\n",
       "      <td>Kurang Darah, IHSG ke Zona Merah Pagi Ini</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5424</th>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>The Fed Bakal Beri Kejutan, Diprediksi Pangkas...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5425 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date                                              Title  \\\n",
       "0     2020-01-01  Mengenal Lagi Naturalisasi, Cara Anies Basweda...   \n",
       "1     2020-01-01  Streaming! Upaya Sarinah Menjadi Pusat Pengemb...   \n",
       "2     2020-01-01  Terungkap! Pembeli Pejaten Village Juga Invest...   \n",
       "3     2020-01-01  Perhatikan 5 Pertanyaan Ini Sebelum Merancang ...   \n",
       "4     2020-01-02                Saham Blue Chip Bisa Digoreng Juga?   \n",
       "...          ...                                                ...   \n",
       "5420  2024-09-25  IHSG Diprediksi Menguat, Simak Analisis dan Re...   \n",
       "5421  2024-09-25  Bikin Investor Happy, The Fed Diprediksi Bakal...   \n",
       "5422  2024-09-26  Suku Bunga Turun, Kemenperin Optimis Iklim Usa...   \n",
       "5423  2024-09-26          Kurang Darah, IHSG ke Zona Merah Pagi Ini   \n",
       "5424  2024-09-30  The Fed Bakal Beri Kejutan, Diprediksi Pangkas...   \n",
       "\n",
       "      Lexicon Sentiment Score  \n",
       "0                           2  \n",
       "1                           7  \n",
       "2                          -1  \n",
       "3                           2  \n",
       "4                           0  \n",
       "...                       ...  \n",
       "5420                        7  \n",
       "5421                        5  \n",
       "5422                        2  \n",
       "5423                        1  \n",
       "5424                        5  \n",
       "\n",
       "[5425 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "df.to_csv('../../Data/lexicon_sentiment_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date  Lexicon Sentiment Score\n",
      "0    2020-01-01                 2.500000\n",
      "1    2020-01-02                 1.666667\n",
      "2    2020-01-03                 6.000000\n",
      "3    2020-01-04                 0.000000\n",
      "4    2020-01-05                 0.000000\n",
      "...         ...                      ...\n",
      "1730 2024-09-26                 1.500000\n",
      "1731 2024-09-27                -0.146667\n",
      "1732 2024-09-28                 1.316667\n",
      "1733 2024-09-29                 1.633333\n",
      "1734 2024-09-30                 5.000000\n",
      "\n",
      "[1735 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the saved dataframe\n",
    "df = pd.read_csv('../../Data/lexicon_sentiment_score.csv')\n",
    "\n",
    "# Convert the Date column to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Calculate the average sentiment score for each day\n",
    "average_sentiment_score = df.groupby('Date')['Lexicon Sentiment Score'].mean()\n",
    "\n",
    "# Create a complete date range\n",
    "start_date = average_sentiment_score.index.min()\n",
    "end_date = average_sentiment_score.index.max()\n",
    "all_dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Reindex the Series with the complete date range\n",
    "average_sentiment_filled = average_sentiment_score.reindex(all_dates)\n",
    "\n",
    "# Create a feature based on the date (day of the year)\n",
    "average_sentiment_filled = average_sentiment_filled.to_frame()  # Convert Series to DataFrame for further processing\n",
    "average_sentiment_filled['day_of_year'] = average_sentiment_filled.index.dayofyear\n",
    "\n",
    "# Normalize the day of the year feature\n",
    "scaler = MinMaxScaler()\n",
    "average_sentiment_filled['day_of_year_scaled'] = scaler.fit_transform(average_sentiment_filled[['day_of_year']])\n",
    "\n",
    "# Prepare data for KNN imputer\n",
    "X = average_sentiment_filled[['day_of_year_scaled', 'Lexicon Sentiment Score']].values\n",
    "\n",
    "# Initialize and fit KNN imputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Create a new DataFrame with imputed values\n",
    "df_imputed = pd.DataFrame(X_imputed, columns=['day_of_year_scaled', 'Lexicon Sentiment Score'], index=average_sentiment_filled.index)\n",
    "\n",
    "# Keep only the sentiment column and reset index\n",
    "df_imputed = df_imputed[['Lexicon Sentiment Score']].reset_index()\n",
    "\n",
    "# Rename the index column to 'Date'\n",
    "df_imputed.columns = ['Date', 'Lexicon Sentiment Score']\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "df_imputed.to_csv('../../Data/lexicon_average_sentiment_score_imputed.csv', index=False, date_format='%Y-%m-%d')\n",
    "\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
